{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc7500a-f485-4f2f-a62f-1720d5f305ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image, ImageDraw, ImageColor\n",
    "import numpy as np\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "def draw_masks(image, masks):\n",
    "    # Create a color map for the masks\n",
    "    color_map = {}\n",
    "    for i in range(masks.shape[0]):\n",
    "        color_map[i] = ImageColor.getrgb(f'hsl({i/float(masks.shape[0])*360},100%,50%)')\n",
    "\n",
    "    # Create a transparent image to overlay the masks on\n",
    "    mask_overlay = Image.new('RGBA', image.size, (0, 0, 0, 0))\n",
    "\n",
    "    # Loop over the masks and draw them on the mask overlay\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask = masks[i, :, :]\n",
    "        color = color_map[i]\n",
    "        mask_overlay_draw = ImageDraw.Draw(mask_overlay)\n",
    "        mask_overlay_draw.bitmap((0, 0), transforms.ToPILImage()(mask), fill=color)\n",
    "\n",
    "    # Blend the mask overlay with the original image using alpha blending\n",
    "    image_alpha = image.copy().convert('RGBA')\n",
    "    image_alpha.putalpha(128)\n",
    "    image_blend = Image.alpha_composite(image_alpha, mask_overlay)\n",
    "\n",
    "    return image_blend.convert('RGB')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fb229-2fa6-451e-8b13-5ce0b17f7287",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Define the COCO dataset and data loader\n",
    "coco_dataset = torchvision.datasets.CocoDetection(root='etc/val2017/', annFile='etc/annotations/captions_val2017.json',\n",
    "                                                  transform=transforms.ToTensor())\n",
    "coco_loader = DataLoader(coco_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load the pre-trained Mask R-CNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1cd7a-715c-4a55-b039-64a796aa6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the images in the dataset and process them with the model\n",
    "for i, (image, target) in enumerate(coco_loader):\n",
    "    # Convert the image to a PIL Image object\n",
    "    image = transforms.ToPILImage()(image[0])\n",
    "    \n",
    "    # Run the image through the model to obtain the predictions\n",
    "    with torch.no_grad():\n",
    "        prediction = model([transforms.ToTensor()(image)])\n",
    "        \n",
    "        \n",
    "        # Convert the tensor to a numpy array\n",
    "        image_np = torch.Tensor.cpu(transforms.ToTensor()(image)).numpy()\n",
    "        # Convert the numpy array to uint8\n",
    "        image_np_uint8 = (image_np * 255).astype('uint8')\n",
    "        result = draw_segmentation_masks(torch.from_numpy(image_np_uint8),\n",
    "                                         masks = prediction[0]['masks'].squeeze(1) > 0.5,\n",
    "                                         alpha=0.9)\n",
    "        show(result)\n",
    "        #masks = prediction[0]['masks'] > 0.5  # threshold the masks\n",
    "        #image_with_masks = draw_masks(image, masks)\n",
    "\n",
    "        # Plot the image with the masks\n",
    "        #image_with_masks.show()\n",
    "    if i == 3:\n",
    "        break\n",
    "    # Print the predicted class labels and bounding boxes for the image\n",
    "    #print(prediction[0]['labels'])\n",
    "    #print(prediction[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675fb50-bcb0-4b28-b320-78db7da03d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('etc/room.jpg')\n",
    "# Downscale the image\n",
    "new_size = (int(image.size[0]/3), int(image.size[1]/3))\n",
    "image = image.resize(new_size)\n",
    "#image = result\n",
    "# Run the image through the model to obtain the predictions\n",
    "with torch.no_grad():\n",
    "    prediction = model([transforms.ToTensor()(image)])\n",
    "    indices = torch.nonzero(prediction[0]['scores'] > 0.90, as_tuple=False).squeeze(1)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    image_np = torch.Tensor.cpu(transforms.ToTensor()(image)).numpy()\n",
    "    # Convert the numpy array to uint8\n",
    "    image_np_uint8 = (image_np * 255).astype('uint8')\n",
    "    result = draw_segmentation_masks(torch.from_numpy(image_np_uint8),\n",
    "                                     masks = prediction[0]['masks'][indices].squeeze(1) > 0.5,\n",
    "                                     alpha=0.9)\n",
    "    \n",
    "    #result = draw_bounding_boxes(torch.from_numpy(image_np_uint8),\n",
    "    #                             prediction[0]['boxes'][indices])\n",
    "    show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21876d19-d64a-4787-a705-a8d8553135e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model([transforms\u001b[38;5;241m.\u001b[39mToTensor()(image)])\n\u001b[1;32m     31\u001b[0m prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msort_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tracking(prediction)\n\u001b[0;32m---> 33\u001b[0m tracked_objects \u001b[38;5;241m=\u001b[39m \u001b[43mmot_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msort_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m detections[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     36\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.80\u001b[39m, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Drone/stream-pytorch-processing/sort.py:208\u001b[0m, in \u001b[0;36mSort.update\u001b[0;34m(self, dets)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(to_del):\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mpop(t)\n\u001b[0;32m--> 208\u001b[0m matched, unmatched_dets, unmatched_trks \u001b[38;5;241m=\u001b[39m \u001b[43massociate_detections_to_trackers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m#update matched trackers with assigned detections\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t,trk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers):\n",
      "File \u001b[0;32m~/Drone/stream-pytorch-processing/sort.py:142\u001b[0m, in \u001b[0;36massociate_detections_to_trackers\u001b[0;34m(detections, trackers, iou_threshold)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mAssigns detections to tracked object (both represented as bounding boxes)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mReturns 3 lists of matches, unmatched_detections and unmatched_trackers\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(trackers)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m), np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    143\u001b[0m iou_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(detections),\u001b[38;5;28mlen\u001b[39m(trackers)),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d,det \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(detections):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"etc/vidroom2.mp4\")\n",
    "\n",
    "# Define the codec and output format for the processed video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "fps = 25.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, fps, (width, height))\n",
    "def tracking(prediction):\n",
    "    sort_list = np.array([])\n",
    "    for i in range(len(prediction[0]['boxes'])):\n",
    "        sort_list = np.append(sort_list,\n",
    "                              np.append(prediction[0]['boxes'][i].detach().cpu().numpy(),\n",
    "                                        prediction[0]['scores'][i].detach().cpu().numpy()))\n",
    "        \n",
    "# Create a VLC HTTP or RTP stream for the output video\n",
    "# Example for RTP:\n",
    "#vlc_url = \"rtp://127.0.0.1:1234/out_stream\"\n",
    "# Example for HTTP:\n",
    "# vlc_url = \"http://127.0.0.1:8080/\"\n",
    "mot_tracker = Sort()\n",
    "# Loop over the frames in the stream, process them, and write them to the output video\n",
    "while cap.isOpened():\n",
    "    # Read the next frame from the stream\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 0)\n",
    "    image = Image.fromarray(frame)\n",
    "    new_size = (int(image.size[0]/2), int(image.size[1]/2))\n",
    "    image = image.resize(new_size)\n",
    "    prediction = model([transforms.ToTensor()(image)])\n",
    "    prediction[0]['sort_list'] = tracking(prediction)\n",
    "\n",
    "    tracked_objects = mot_tracker.update(prediction[0]['sort_list'] )\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    \n",
    "    indices = torch.nonzero(prediction[0]['scores'] > 0.80, as_tuple=False).squeeze(1)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    image_np = torch.Tensor.cpu(transforms.ToTensor()(image)).numpy()\n",
    "    # Convert the numpy array to uint8\n",
    "    image_np_uint8 = (image_np * 255).astype('uint8')\n",
    "    result = draw_segmentation_masks(torch.from_numpy(image_np_uint8),\n",
    "                                     masks = prediction[0]['masks'][indices].squeeze(1) > 0.5,\n",
    "                                     alpha=0.9)\n",
    "    \n",
    "    #result = draw_bounding_boxes(torch.from_numpy(image_np_uint8),\n",
    "    #                             prediction[0]['boxes'][indices])\n",
    "    show(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17bb134c-8d8d-4529-9dbf-fd4b6df7f5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[1.3702e+02, 1.6410e+02, 2.6870e+02, 3.2760e+02],\n",
       "          [1.8343e+02, 5.7907e+02, 3.9327e+02, 7.3983e+02],\n",
       "          [3.0108e+02, 2.4881e+02, 3.5896e+02, 3.2978e+02],\n",
       "          [9.8902e+01, 4.6005e+02, 1.3199e+02, 5.0768e+02],\n",
       "          [4.6913e+01, 4.4941e+02, 7.9249e+01, 4.9693e+02],\n",
       "          [2.2895e+02, 3.6977e+02, 2.6802e+02, 4.5983e+02],\n",
       "          [1.8956e+02, 2.6882e+02, 2.4881e+02, 3.2601e+02],\n",
       "          [2.6052e+02, 2.5728e+02, 3.1644e+02, 3.2704e+02],\n",
       "          [1.4334e+02, 4.3628e+02, 1.8993e+02, 4.9396e+02],\n",
       "          [1.3913e+02, 3.7177e+02, 1.9005e+02, 4.9426e+02],\n",
       "          [9.0604e+01, 4.5518e+02, 1.2667e+02, 4.8898e+02],\n",
       "          [3.7486e+02, 6.1336e+02, 5.3845e+02, 8.4781e+02],\n",
       "          [4.1288e+02, 2.9017e+02, 4.7749e+02, 3.5335e+02],\n",
       "          [3.7624e+02, 1.4689e+02, 5.3005e+02, 3.4583e+02],\n",
       "          [4.9643e+00, 4.6168e+02, 1.9992e+02, 6.7923e+02],\n",
       "          [1.1329e+02, 4.5594e+02, 1.4185e+02, 4.9080e+02],\n",
       "          [1.4484e+02, 4.4137e+02, 1.9014e+02, 4.9521e+02],\n",
       "          [3.3482e+02, 3.9683e+02, 3.8825e+02, 4.5817e+02],\n",
       "          [1.7904e+02, 4.5467e+02, 4.5439e+02, 7.1507e+02],\n",
       "          [2.6155e+02, 2.9541e+02, 2.8292e+02, 3.2615e+02],\n",
       "          [4.0033e+02, 1.9781e+02, 5.0389e+02, 3.5211e+02],\n",
       "          [3.6076e+02, 2.0155e+02, 4.5047e+02, 3.3290e+02],\n",
       "          [8.8723e+01, 4.5452e+02, 1.0808e+02, 4.8680e+02],\n",
       "          [2.9949e+02, 2.9697e+02, 3.4497e+02, 3.2924e+02],\n",
       "          [1.2509e+02, 4.5989e+02, 1.4237e+02, 4.8905e+02],\n",
       "          [1.8235e+02, 3.4293e+02, 5.3338e+02, 7.6654e+02],\n",
       "          [3.3854e+02, 2.9285e+02, 3.6240e+02, 3.3075e+02],\n",
       "          [2.1093e+02, 3.7664e+02, 2.3631e+02, 4.4088e+02],\n",
       "          [7.7343e+01, 4.5276e+02, 1.0057e+02, 4.8407e+02],\n",
       "          [2.7289e+02, 2.4575e+02, 3.7859e+02, 3.3641e+02],\n",
       "          [2.9759e+02, 2.7693e+02, 3.4705e+02, 3.2991e+02],\n",
       "          [3.5125e+01, 3.9677e+02, 5.3532e+02, 9.3661e+02],\n",
       "          [2.1721e+02, 3.9530e+02, 2.3027e+02, 4.3912e+02],\n",
       "          [2.0324e+02, 4.3978e+02, 2.2859e+02, 4.6410e+02],\n",
       "          [2.6601e+02, 2.8410e+02, 3.0719e+02, 3.2748e+02],\n",
       "          [3.6634e+02, 3.0432e+02, 3.9228e+02, 3.3257e+02],\n",
       "          [3.0125e+02, 2.1297e+02, 4.3213e+02, 3.3714e+02],\n",
       "          [4.1645e+02, 2.5758e+02, 4.9143e+02, 3.4917e+02],\n",
       "          [3.6082e+02, 3.0191e+02, 3.7705e+02, 3.3183e+02],\n",
       "          [3.9224e+02, 2.7699e+02, 4.2572e+02, 3.3251e+02],\n",
       "          [1.3567e+02, 1.1680e+02, 5.3320e+02, 6.9962e+02],\n",
       "          [3.5006e+02, 2.1511e+02, 4.1605e+02, 3.3449e+02],\n",
       "          [3.0664e+00, 4.6156e+02, 2.0746e+02, 6.7701e+02],\n",
       "          [2.3734e+02, 3.6826e+02, 2.6576e+02, 3.9480e+02],\n",
       "          [1.0441e+02, 4.5448e+02, 1.3054e+02, 4.6671e+02],\n",
       "          [6.7392e+01, 4.5136e+02, 1.1387e+02, 4.8805e+02],\n",
       "          [1.1525e+02, 4.4868e+02, 1.3507e+02, 4.6372e+02],\n",
       "          [1.4043e+02, 4.2052e+02, 1.8822e+02, 4.6722e+02],\n",
       "          [2.1201e+02, 3.7626e+02, 2.3847e+02, 3.9826e+02],\n",
       "          [5.9953e+01, 4.5086e+02, 1.4708e+02, 4.9760e+02],\n",
       "          [3.6498e+02, 2.8489e+02, 4.0186e+02, 3.3215e+02],\n",
       "          [2.7760e+02, 1.7218e+02, 4.7790e+02, 3.4461e+02],\n",
       "          [2.0141e+02, 6.0306e+02, 5.1830e+02, 8.5892e+02],\n",
       "          [3.5327e+02, 1.5636e+02, 5.3711e+02, 6.6864e+02],\n",
       "          [1.0847e+02, 4.5344e+02, 1.3930e+02, 4.7288e+02],\n",
       "          [1.4772e+02, 4.2717e+02, 1.8643e+02, 4.5460e+02],\n",
       "          [1.2492e+02, 1.7384e+02, 2.8201e+02, 4.3109e+02],\n",
       "          [2.8363e+02, 5.9106e+02, 3.2740e+02, 6.4613e+02],\n",
       "          [1.3477e+02, 5.5857e+02, 5.4000e+02, 8.6230e+02],\n",
       "          [3.4469e+02, 2.6271e+02, 4.0353e+02, 3.3270e+02],\n",
       "          [1.2543e+02, 1.4384e+02, 5.2481e+02, 3.7020e+02],\n",
       "          [4.1438e+02, 2.8995e+02, 4.7869e+02, 3.5289e+02],\n",
       "          [3.8777e+02, 6.1329e+02, 5.3313e+02, 7.2857e+02],\n",
       "          [2.7865e+02, 4.0701e+02, 3.2125e+02, 4.5884e+02],\n",
       "          [4.0553e+02, 1.6394e+02, 5.3260e+02, 3.1400e+02],\n",
       "          [2.3177e+02, 4.7133e+02, 3.3805e+02, 6.5117e+02],\n",
       "          [3.3940e+02, 2.8961e+02, 3.6191e+02, 3.3153e+02],\n",
       "          [9.7330e-02, 6.1184e+02, 1.8347e+01, 7.0703e+02]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([64, 86, 64, 47, 47, 44, 86, 64, 86, 64, 47, 62, 86, 64, 79, 47, 47, 86,\n",
       "          64, 86, 64, 64, 47, 86, 47, 64, 86, 44, 47, 64, 64, 67, 44, 47, 86, 86,\n",
       "          64, 86, 86, 86, 64, 64, 78, 44, 47, 47, 47, 86, 44, 47, 86, 64, 62, 64,\n",
       "          47, 86, 64, 86, 67, 64, 64, 47, 86, 86, 28, 64, 44,  3]),\n",
       "  'scores': tensor([0.9875, 0.9809, 0.9786, 0.9594, 0.9517, 0.9497, 0.9373, 0.9238, 0.8926,\n",
       "          0.8642, 0.8521, 0.8452, 0.8451, 0.8217, 0.7902, 0.7731, 0.7137, 0.6836,\n",
       "          0.6661, 0.6058, 0.5987, 0.5373, 0.5037, 0.4843, 0.4453, 0.4351, 0.4180,\n",
       "          0.3883, 0.3773, 0.3579, 0.3303, 0.3182, 0.2783, 0.2599, 0.2344, 0.2216,\n",
       "          0.2078, 0.1956, 0.1872, 0.1803, 0.1761, 0.1656, 0.1568, 0.1542, 0.1499,\n",
       "          0.1244, 0.1219, 0.1146, 0.1099, 0.1033, 0.1012, 0.1008, 0.0967, 0.0917,\n",
       "          0.0867, 0.0844, 0.0831, 0.0828, 0.0821, 0.0768, 0.0698, 0.0683, 0.0665,\n",
       "          0.0652, 0.0584, 0.0559, 0.0548, 0.0544], grad_fn=<IndexBackward0>),\n",
       "  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efcf2b5-4bf5-4ff0-a6f1-16dc26bd2fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee392cf3-1ad1-4277-b764-853b6ac1ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image, ImageDraw, ImageColor\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sort import Sort\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5995604f-5c83-4af6-a9f4-1b4379c9a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmt/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cmt/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def load_maskrcnn():\n",
    "    # Define the COCO dataset and data loader\n",
    "    coco_dataset = torchvision.datasets.CocoDetection(root='etc/val2017/', annFile='etc/annotations/captions_val2017.json',\n",
    "                                                      transform=transforms.ToTensor())\n",
    "    coco_loader = DataLoader(coco_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Load the pre-trained Mask R-CNN model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def feedforward(model, image):\n",
    "    # Convert the frame to a NumPy array\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # change color space if necessary\n",
    "    image = Image.fromarray(image)\n",
    "    new_size = (int(image.size[0]/2), int(image.size[1]/2))\n",
    "    image = image.resize(new_size)\n",
    "    prediction = model([transforms.ToTensor()(image)])\n",
    "    indices = torch.nonzero(prediction[0]['scores'] > 0.90, as_tuple=False).squeeze(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the tensor to a numpy array\n",
    "    image_np = torch.Tensor.cpu(transforms.ToTensor()(image)).numpy()\n",
    "    # Convert the numpy array to uint8\n",
    "    image_np_uint8 = (image_np * 255).astype('uint8')\n",
    "    result = draw_segmentation_masks(torch.from_numpy(image_np_uint8),\n",
    "                                     masks = prediction[0]['masks'][indices].squeeze(1) > 0.5,\n",
    "                                     alpha=0.9)\n",
    "\n",
    "    result = result.detach().cpu().numpy()\n",
    "    result = np.rollaxis(result, 0, 3)\n",
    "    #result = F.to_pil_image(result)\n",
    "\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def PIL_to_cv2(image: np.array):    \n",
    "    # Convert the PIL image to a NumPy array in BGR format\n",
    "    output_np = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return output_np\n",
    "\n",
    "\n",
    "# Connect to a live stream\n",
    "stream_url = \"rtmp://192.168.25.1:8082/live\"\n",
    "model = load_maskrcnn()\n",
    "\n",
    "# Create an OpenCV video capture object to decode the frames from the stream\n",
    "#cap = cv2.VideoCapture(stream_url)\n",
    "cap = cv2.VideoCapture(\"etc/vidroom2.mp4\")\n",
    "\n",
    "# Define the codec and output format for the processed video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "fps = 25.0\n",
    "#width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "width = 540\n",
    "#height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "height = 960\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, fps, (width, height))\n",
    "\n",
    "# Create a VLC HTTP or RTP stream for the output video\n",
    "# Example for RTP:\n",
    "#vlc_url = \"rtp://127.0.0.1:1234/out_stream\"\n",
    "# Example for HTTP:\n",
    "# vlc_url = \"http://127.0.0.1:8080/\"\n",
    "\n",
    "# Loop over the frames in the stream, process them, and write them to the output video\n",
    "counter = 0\n",
    "while cap.isOpened():\n",
    "    # Read the next frame from the stream\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 0)\n",
    "    if not ret:\n",
    "        break\n",
    "    # Process the frame with PyTorch\n",
    "    result = feedforward(model, frame)\n",
    "    #result_frame = PIL_to_cv2(result)\n",
    "    # Write the processed frame to the output video\n",
    "    out.write(result)\n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        out.release()\n",
    "        cap.release()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5e2c2-8d35-45a4-a6a5-bc6e906ab705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
