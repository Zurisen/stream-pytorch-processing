{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74e7d60-4597-46aa-9aca-b0c7cef96031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def6773f-6379-4210-a26c-382d8573e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cmt/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-4-1 Python-3.10.9 torch-1.12.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "from sort import *\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab957ae1-c8d0-4339-85d2-3090c2a0b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.eval()\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854d8579-e0b4-430e-8012-00d37b13e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=416\n",
    "conf_thres=0.8\n",
    "nms_thres=0.4\n",
    "def detect_image(img):\n",
    "    # scale and pad image\n",
    "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
    "    imw = round(img.size[0] * ratio)\n",
    "    imh = round(img.size[1] * ratio)\n",
    "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                        (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0).to(device)\n",
    "    #input_img = Variable(image_tensor.type(Tensor))\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(image_tensor)\n",
    "        #detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    return detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a0755-dbaa-47cc-8b73-a7285ee3beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = 'etc/vidroom2.mp4'\n",
    "\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# initialize Sort object and video capture\n",
    "vid = cv2.VideoCapture(videopath)\n",
    "mot_tracker = Sort() \n",
    "\n",
    "# Save output to avi video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "fps = 25.0\n",
    "width = 1080\n",
    "height = 1920\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, fps, (width, height))\n",
    "\n",
    "#while(True):\n",
    "counter = 0\n",
    "while vid.isOpened():\n",
    "    print(\"Frame \", counter)\n",
    "    ret, frame = vid.read()\n",
    "    if frame is None:\n",
    "        print('Finished!')\n",
    "        out.release()\n",
    "        break\n",
    "    counter += 1\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.flip(frame, 0)\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    detections = model(frame)\n",
    "    #print(\"N det: \", len(detections.pandas().xyxy[0]))\n",
    "\n",
    "    img = np.array(pilimg)\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "    #detections.render()  # updates results.ims with boxes and labels\n",
    "    #for im in detections.ims:\n",
    "    #    buffered = BytesIO()\n",
    "    #    im_base64 = Image.fromarray(im)\n",
    "    #frame = cv2.cvtColor(numpy.array(im_base64), cv2.COLOR_RGB2BGR)\n",
    "    if detections is not None:\n",
    "        print(\"N det: \", len(detections.pandas().xyxy[0]))\n",
    "        tracked_objects = mot_tracker.update(detections.pandas().xyxy[0].iloc[:,:5].to_numpy())\n",
    "        print(\"N tracked: \", len(tracked_objects))\n",
    "\n",
    "        for x1, y1, x2, y2, obj_id in tracked_objects:\n",
    "            #x1, y1, x2, y2, obj_id = float(x1), float(y1), float(x2), float(y2), int(obj_id)\n",
    "            #box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "            #box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "            #y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "            #x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "            color = colors[int(obj_id) % len(colors)]\n",
    "            color = [i * 255 for i in color]\n",
    "            #cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
    "            #cv2.rectangle(frame, (x1, y1-35), (x1+len(str(obj_id))*19+60, y1), color, -1)\n",
    "            #cv2.putText(frame, str(obj_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
    "            \n",
    "            # Draw the bounding box around the tracked object\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
    "            # Save the detections to the video file\n",
    "            #out.write(frame)\n",
    "    out.write(frame)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6216198-db92-4b19-81f3-0bdef671c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0205df-96d8-4002-b510-803e63eda90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708e63fc-0679-4b78-9814-8cc6f9fb70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a360478-9b6d-42f7-b66d-8bfc60019913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
