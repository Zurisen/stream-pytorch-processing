{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def mediapipeDetect(image):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose()\n",
    "    results = pose.process(image)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Draw hollow circles for each landmark\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            x = int(landmark.x * image.shape[1])\n",
    "            y = int(landmark.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 5, (0, 255, 0), 1)  # Hollow circle with thin edge\n",
    "        \n",
    "        # Draw lines to connect the landmarks and form the skeleton\n",
    "        landmark_points = [(int(lm.x * image.shape[1]), int(lm.y * image.shape[0])) for lm in results.pose_landmarks.landmark]\n",
    "        connections = mp_pose.POSE_CONNECTIONS\n",
    "        for connection in connections:\n",
    "            start_idx, end_idx = connection\n",
    "            start_point = landmark_points[start_idx]\n",
    "            end_point = landmark_points[end_idx]\n",
    "            cv2.line(image, start_point, end_point, (0, 255, 0), 2)  # Line connecting the landmarks\n",
    "    \n",
    "    cv2.imshow(\"MediaPipe Pose Detection\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 130.2ms\n",
      "Speed: 2.2ms preprocess, 130.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 91.7ms\n",
      "Speed: 1.5ms preprocess, 91.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 88.3ms\n",
      "Speed: 1.4ms preprocess, 88.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 95.9ms\n",
      "Speed: 1.6ms preprocess, 95.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 101.4ms\n",
      "Speed: 1.7ms preprocess, 101.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 89.6ms\n",
      "Speed: 1.4ms preprocess, 89.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 102.5ms\n",
      "Speed: 1.5ms preprocess, 102.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 90.2ms\n",
      "Speed: 1.5ms preprocess, 90.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 89.3ms\n",
      "Speed: 1.4ms preprocess, 89.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 89.9ms\n",
      "Speed: 1.9ms preprocess, 89.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 93.0ms\n",
      "Speed: 1.5ms preprocess, 93.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 165.1ms\n",
      "Speed: 1.6ms preprocess, 165.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 136.2ms\n",
      "Speed: 2.4ms preprocess, 136.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 132.6ms\n",
      "Speed: 2.4ms preprocess, 132.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 168.4ms\n",
      "Speed: 2.9ms preprocess, 168.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 103.2ms\n",
      "Speed: 1.2ms preprocess, 103.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 114.2ms\n",
      "Speed: 1.5ms preprocess, 114.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 117.8ms\n",
      "Speed: 2.0ms preprocess, 117.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 108.8ms\n",
      "Speed: 1.5ms preprocess, 108.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 107.7ms\n",
      "Speed: 1.6ms preprocess, 107.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 108.5ms\n",
      "Speed: 2.0ms preprocess, 108.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 87.7ms\n",
      "Speed: 1.4ms preprocess, 87.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 86.6ms\n",
      "Speed: 1.2ms preprocess, 86.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 90.5ms\n",
      "Speed: 1.5ms preprocess, 90.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 94.0ms\n",
      "Speed: 1.2ms preprocess, 94.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 92.4ms\n",
      "Speed: 1.1ms preprocess, 92.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 93.2ms\n",
      "Speed: 1.6ms preprocess, 93.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 105.5ms\n",
      "Speed: 2.0ms preprocess, 105.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 106.5ms\n",
      "Speed: 1.6ms preprocess, 106.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 90.1ms\n",
      "Speed: 1.3ms preprocess, 90.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 110.3ms\n",
      "Speed: 1.8ms preprocess, 110.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 85.0ms\n",
      "Speed: 1.2ms preprocess, 85.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 92.3ms\n",
      "Speed: 1.5ms preprocess, 92.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 93.8ms\n",
      "Speed: 1.8ms preprocess, 93.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 107.4ms\n",
      "Speed: 1.9ms preprocess, 107.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 107.3ms\n",
      "Speed: 1.4ms preprocess, 107.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 122.1ms\n",
      "Speed: 1.7ms preprocess, 122.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 123.7ms\n",
      "Speed: 1.7ms preprocess, 123.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 120.7ms\n",
      "Speed: 1.6ms preprocess, 120.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 109.5ms\n",
      "Speed: 1.4ms preprocess, 109.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 108.8ms\n",
      "Speed: 1.9ms preprocess, 108.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 108.9ms\n",
      "Speed: 1.4ms preprocess, 108.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 110.0ms\n",
      "Speed: 1.2ms preprocess, 110.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 103.9ms\n",
      "Speed: 1.4ms preprocess, 103.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 106.9ms\n",
      "Speed: 1.5ms preprocess, 106.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 99.5ms\n",
      "Speed: 1.4ms preprocess, 99.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 101.3ms\n",
      "Speed: 1.7ms preprocess, 101.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 104.0ms\n",
      "Speed: 1.3ms preprocess, 104.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 110.7ms\n",
      "Speed: 1.5ms preprocess, 110.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 102.7ms\n",
      "Speed: 1.4ms preprocess, 102.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 92.4ms\n",
      "Speed: 1.5ms preprocess, 92.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 90.0ms\n",
      "Speed: 1.3ms preprocess, 90.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 90.2ms\n",
      "Speed: 1.5ms preprocess, 90.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 88.9ms\n",
      "Speed: 1.4ms preprocess, 88.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 96.3ms\n",
      "Speed: 1.4ms preprocess, 96.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 89.9ms\n",
      "Speed: 1.2ms preprocess, 89.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 88.9ms\n",
      "Speed: 1.5ms preprocess, 88.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 87.5ms\n",
      "Speed: 1.5ms preprocess, 87.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 93.7ms\n",
      "Speed: 1.4ms preprocess, 93.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 101.6ms\n",
      "Speed: 1.2ms preprocess, 101.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 99.2ms\n",
      "Speed: 1.3ms preprocess, 99.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 101.7ms\n",
      "Speed: 1.4ms preprocess, 101.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 92.3ms\n",
      "Speed: 1.6ms preprocess, 92.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 90.1ms\n",
      "Speed: 1.1ms preprocess, 90.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 106.5ms\n",
      "Speed: 1.7ms preprocess, 106.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 108.4ms\n",
      "Speed: 1.9ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 94.4ms\n",
      "Speed: 1.5ms preprocess, 94.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 93.2ms\n",
      "Speed: 1.6ms preprocess, 93.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 1 microwave, 104.8ms\n",
      "Speed: 2.1ms preprocess, 104.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 93.9ms\n",
      "Speed: 1.6ms preprocess, 93.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 tvs, 104.9ms\n",
      "Speed: 1.6ms preprocess, 104.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 93.9ms\n",
      "Speed: 1.2ms preprocess, 93.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 microwave, 94.0ms\n",
      "Speed: 1.4ms preprocess, 94.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.5ms\n",
      "Speed: 1.4ms preprocess, 92.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.2ms\n",
      "Speed: 1.8ms preprocess, 98.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 94.0ms\n",
      "Speed: 1.6ms preprocess, 94.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 152.2ms\n",
      "Speed: 31.6ms preprocess, 152.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 118.4ms\n",
      "Speed: 2.1ms preprocess, 118.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 115.9ms\n",
      "Speed: 2.7ms preprocess, 115.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 115.9ms\n",
      "Speed: 2.1ms preprocess, 115.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 108.2ms\n",
      "Speed: 2.3ms preprocess, 108.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 106.3ms\n",
      "Speed: 1.3ms preprocess, 106.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 107.3ms\n",
      "Speed: 1.5ms preprocess, 107.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 108.8ms\n",
      "Speed: 1.5ms preprocess, 108.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.7ms\n",
      "Speed: 1.8ms preprocess, 107.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 111.3ms\n",
      "Speed: 2.0ms preprocess, 111.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 108.4ms\n",
      "Speed: 1.3ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 109.1ms\n",
      "Speed: 1.4ms preprocess, 109.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.1ms\n",
      "Speed: 1.4ms preprocess, 99.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.9ms\n",
      "Speed: 1.6ms preprocess, 98.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 99.2ms\n",
      "Speed: 1.4ms preprocess, 99.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 100.7ms\n",
      "Speed: 1.6ms preprocess, 100.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 119.6ms\n",
      "Speed: 1.5ms preprocess, 119.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 96.5ms\n",
      "Speed: 1.5ms preprocess, 96.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 97.2ms\n",
      "Speed: 1.2ms preprocess, 97.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 100.4ms\n",
      "Speed: 1.7ms preprocess, 100.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 103.5ms\n",
      "Speed: 1.6ms preprocess, 103.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 109.0ms\n",
      "Speed: 1.4ms preprocess, 109.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 101.4ms\n",
      "Speed: 1.4ms preprocess, 101.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 98.0ms\n",
      "Speed: 1.5ms preprocess, 98.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 95.0ms\n",
      "Speed: 1.8ms preprocess, 95.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 100.9ms\n",
      "Speed: 1.3ms preprocess, 100.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 92.0ms\n",
      "Speed: 1.3ms preprocess, 92.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 94.1ms\n",
      "Speed: 1.4ms preprocess, 94.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 93.6ms\n",
      "Speed: 1.2ms preprocess, 93.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 91.5ms\n",
      "Speed: 1.5ms preprocess, 91.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 95.9ms\n",
      "Speed: 1.2ms preprocess, 95.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 92.4ms\n",
      "Speed: 1.3ms preprocess, 92.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 90.6ms\n",
      "Speed: 1.3ms preprocess, 90.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 89.6ms\n",
      "Speed: 1.3ms preprocess, 89.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 91.9ms\n",
      "Speed: 1.5ms preprocess, 91.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 93.8ms\n",
      "Speed: 1.3ms preprocess, 93.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 96.2ms\n",
      "Speed: 1.5ms preprocess, 96.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 128.9ms\n",
      "Speed: 1.8ms preprocess, 128.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 117.2ms\n",
      "Speed: 2.0ms preprocess, 117.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 117.9ms\n",
      "Speed: 1.8ms preprocess, 117.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 102.5ms\n",
      "Speed: 1.5ms preprocess, 102.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 115.2ms\n",
      "Speed: 2.4ms preprocess, 115.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 113.4ms\n",
      "Speed: 1.8ms preprocess, 113.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 109.1ms\n",
      "Speed: 1.4ms preprocess, 109.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 134.0ms\n",
      "Speed: 3.2ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 117.6ms\n",
      "Speed: 1.7ms preprocess, 117.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 118.1ms\n",
      "Speed: 1.6ms preprocess, 118.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 124.8ms\n",
      "Speed: 1.7ms preprocess, 124.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 121.1ms\n",
      "Speed: 1.5ms preprocess, 121.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 111.1ms\n",
      "Speed: 2.2ms preprocess, 111.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 112.2ms\n",
      "Speed: 1.7ms preprocess, 112.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 103.5ms\n",
      "Speed: 1.4ms preprocess, 103.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 99.8ms\n",
      "Speed: 1.3ms preprocess, 99.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 110.9ms\n",
      "Speed: 1.5ms preprocess, 110.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 99.7ms\n",
      "Speed: 1.6ms preprocess, 99.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 99.1ms\n",
      "Speed: 1.4ms preprocess, 99.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 94.6ms\n",
      "Speed: 1.4ms preprocess, 94.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 96.1ms\n",
      "Speed: 1.2ms preprocess, 96.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 1 tv, 95.4ms\n",
      "Speed: 1.2ms preprocess, 95.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 103.1ms\n",
      "Speed: 1.4ms preprocess, 103.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.8ms\n",
      "Speed: 1.4ms preprocess, 99.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 100.9ms\n",
      "Speed: 1.4ms preprocess, 100.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 100.7ms\n",
      "Speed: 4.7ms preprocess, 100.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 111.1ms\n",
      "Speed: 1.5ms preprocess, 111.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 108.8ms\n",
      "Speed: 2.3ms preprocess, 108.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 123.6ms\n",
      "Speed: 1.6ms preprocess, 123.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 126.0ms\n",
      "Speed: 1.5ms preprocess, 126.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 131.9ms\n",
      "Speed: 1.8ms preprocess, 131.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 114.6ms\n",
      "Speed: 2.2ms preprocess, 114.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 112.4ms\n",
      "Speed: 1.6ms preprocess, 112.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 111.9ms\n",
      "Speed: 1.7ms preprocess, 111.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 97.3ms\n",
      "Speed: 1.4ms preprocess, 97.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 tv, 106.9ms\n",
      "Speed: 1.8ms preprocess, 106.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 101.7ms\n",
      "Speed: 5.2ms preprocess, 101.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 108.0ms\n",
      "Speed: 1.3ms preprocess, 108.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 107.6ms\n",
      "Speed: 1.6ms preprocess, 107.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 101.5ms\n",
      "Speed: 1.8ms preprocess, 101.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 102.0ms\n",
      "Speed: 1.4ms preprocess, 102.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 114.0ms\n",
      "Speed: 1.6ms preprocess, 114.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 102.6ms\n",
      "Speed: 1.6ms preprocess, 102.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 109.8ms\n",
      "Speed: 1.4ms preprocess, 109.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 133.6ms\n",
      "Speed: 2.5ms preprocess, 133.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 117.7ms\n",
      "Speed: 2.0ms preprocess, 117.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bowl, 1 tv, 119.2ms\n",
      "Speed: 1.7ms preprocess, 119.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     bounding_box_region \u001b[38;5;241m=\u001b[39m frame[y1:y2, x1:x2]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Display the annotated frame\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmediapipeDetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounding_box_region\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#cv2.imshow(\"YOLO11 Tracking\", bounding_box_region)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Break the loop if 'q' is pressed\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mmediapipeDetect\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      6\u001b[0m mp_pose \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mpose\n\u001b[0;32m      7\u001b[0m pose \u001b[38;5;241m=\u001b[39m mp_pose\u001b[38;5;241m.\u001b[39mPose()\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Draw hollow circles for each landmark\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"etc/drone.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True, conf=0.55)\n",
    "        \n",
    "        # Filter the results to only include boxes with class 0 (person)\n",
    "        filtered_indexes = torch.where(results[0].boxes.cls == 0)\n",
    "        filtered_boxes = None\n",
    "        if filtered_indexes[0].shape[0] > 0:\n",
    "            filtered_boxes = results[0].boxes[filtered_indexes]\n",
    "\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        filtered_results = results[0]\n",
    "        if filtered_boxes is not None:\n",
    "            filtered_results.boxes = filtered_boxes\n",
    "            annotated_frame = filtered_results.plot()\n",
    "            \n",
    "            # Save the bounding box region of the image in another variable\n",
    "            x1, y1, x2, y2 = map(int, filtered_boxes.xyxy[0])\n",
    "            bounding_box_region = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Display the annotated frame\n",
    "        mediapipeDetect(bounding_box_region)\n",
    "        #cv2.imshow(\"YOLO11 Tracking\", bounding_box_region)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 45,  90, 124],\n",
       "        [ 44,  89, 123],\n",
       "        [ 46,  89, 123],\n",
       "        ...,\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161]],\n",
       "\n",
       "       [[ 47,  92, 126],\n",
       "        [ 46,  91, 125],\n",
       "        [ 46,  89, 123],\n",
       "        ...,\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161]],\n",
       "\n",
       "       [[ 49,  97, 130],\n",
       "        [ 47,  95, 128],\n",
       "        [ 46,  91, 125],\n",
       "        ...,\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161],\n",
       "        [ 51, 114, 161]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 43,  43,  57],\n",
       "        [ 45,  45,  59],\n",
       "        [ 86,  42,  45],\n",
       "        ...,\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157]],\n",
       "\n",
       "       [[ 43,  43,  57],\n",
       "        [ 45,  45,  59],\n",
       "        [117,  42,  38],\n",
       "        ...,\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157]],\n",
       "\n",
       "       [[ 43,  43,  57],\n",
       "        [ 45,  45,  59],\n",
       "        [ 46,  42,  56],\n",
       "        ...,\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157],\n",
       "        [ 50, 112, 157]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: https://youtu.be/OO7XT24AmTY?si=w-_UTTuJr5Fj1Syl... Success  (4593 frames of shape 1920x1080 at 29.97 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 736x1280 (no detections), 269.4ms\n",
      "0: 736x1280 (no detections), 189.3ms\n",
      "0: 736x1280 (no detections), 201.5ms\n",
      "0: 736x1280 (no detections), 199.2ms\n",
      "0: 736x1280 (no detections), 209.6ms\n",
      "0: 736x1280 1 person, 197.1ms\n",
      "0: 736x1280 1 person, 197.9ms\n",
      "0: 736x1280 1 person, 192.0ms\n",
      "0: 736x1280 1 person, 196.9ms\n",
      "0: 736x1280 (no detections), 193.3ms\n",
      "0: 736x1280 (no detections), 193.8ms\n",
      "0: 736x1280 (no detections), 210.2ms\n",
      "0: 736x1280 (no detections), 196.7ms\n",
      "0: 736x1280 (no detections), 193.3ms\n",
      "0: 736x1280 (no detections), 204.4ms\n",
      "0: 736x1280 (no detections), 176.0ms\n",
      "0: 736x1280 (no detections), 190.6ms\n",
      "0: 736x1280 (no detections), 183.5ms\n",
      "0: 736x1280 (no detections), 180.3ms\n",
      "0: 736x1280 (no detections), 171.1ms\n",
      "0: 736x1280 (no detections), 157.6ms\n",
      "0: 736x1280 (no detections), 155.5ms\n",
      "0: 736x1280 (no detections), 162.6ms\n",
      "0: 736x1280 (no detections), 160.2ms\n",
      "0: 736x1280 (no detections), 154.8ms\n",
      "0: 736x1280 (no detections), 166.0ms\n",
      "0: 736x1280 (no detections), 159.8ms\n",
      "0: 736x1280 (no detections), 147.2ms\n",
      "WARNING  Waiting for stream 0\n",
      "WARNING  Waiting for stream 0\n",
      "WARNING  Waiting for stream 0\n",
      "0: 736x1280 (no detections), 165.0ms\n",
      "0: 736x1280 (no detections), 171.7ms\n",
      "0: 736x1280 (no detections), 163.8ms\n",
      "0: 736x1280 (no detections), 160.4ms\n",
      "0: 736x1280 (no detections), 163.4ms\n",
      "0: 736x1280 (no detections), 159.5ms\n",
      "0: 736x1280 (no detections), 165.3ms\n",
      "0: 736x1280 (no detections), 166.7ms\n",
      "0: 736x1280 (no detections), 164.1ms\n",
      "0: 736x1280 (no detections), 164.5ms\n",
      "0: 736x1280 (no detections), 166.9ms\n",
      "0: 736x1280 1 person, 164.1ms\n",
      "0: 736x1280 1 person, 164.2ms\n",
      "0: 736x1280 (no detections), 152.7ms\n",
      "0: 736x1280 (no detections), 166.4ms\n",
      "0: 736x1280 (no detections), 165.0ms\n",
      "0: 736x1280 1 person, 156.5ms\n",
      "0: 736x1280 1 person, 168.9ms\n",
      "0: 736x1280 1 person, 166.9ms\n",
      "0: 736x1280 (no detections), 167.9ms\n",
      "0: 736x1280 (no detections), 161.6ms\n",
      "0: 736x1280 (no detections), 175.1ms\n",
      "0: 736x1280 (no detections), 163.6ms\n",
      "0: 736x1280 (no detections), 174.4ms\n",
      "0: 736x1280 1 person, 170.8ms\n",
      "0: 736x1280 1 person, 168.8ms\n",
      "0: 736x1280 1 person, 167.6ms\n",
      "0: 736x1280 1 person, 161.8ms\n",
      "0: 736x1280 1 person, 166.8ms\n",
      "0: 736x1280 1 person, 179.8ms\n",
      "0: 736x1280 1 person, 170.5ms\n",
      "0: 736x1280 1 person, 179.7ms\n",
      "0: 736x1280 1 person, 163.3ms\n",
      "0: 736x1280 1 person, 166.8ms\n",
      "0: 736x1280 (no detections), 215.9ms\n",
      "0: 736x1280 1 person, 178.8ms\n",
      "0: 736x1280 1 person, 178.2ms\n",
      "0: 736x1280 1 person, 151.7ms\n",
      "0: 736x1280 1 person, 177.2ms\n",
      "0: 736x1280 1 person, 174.6ms\n",
      "0: 736x1280 1 person, 178.2ms\n",
      "0: 736x1280 1 person, 166.9ms\n",
      "0: 736x1280 1 person, 169.1ms\n",
      "0: 736x1280 2 persons, 171.9ms\n",
      "0: 736x1280 2 persons, 174.5ms\n",
      "0: 736x1280 1 person, 169.6ms\n",
      "0: 736x1280 1 person, 151.9ms\n",
      "0: 736x1280 2 persons, 179.9ms\n",
      "0: 736x1280 1 person, 177.0ms\n",
      "0: 736x1280 1 person, 172.2ms\n",
      "0: 736x1280 2 persons, 175.6ms\n",
      "0: 736x1280 1 person, 176.5ms\n",
      "0: 736x1280 1 person, 171.8ms\n",
      "0: 736x1280 2 persons, 173.2ms\n",
      "0: 736x1280 2 persons, 173.0ms\n",
      "0: 736x1280 2 persons, 159.4ms\n",
      "0: 736x1280 2 persons, 189.1ms\n",
      "0: 736x1280 2 persons, 169.6ms\n",
      "0: 736x1280 2 persons, 175.4ms\n",
      "0: 736x1280 1 person, 170.4ms\n",
      "0: 736x1280 2 persons, 170.6ms\n",
      "0: 736x1280 2 persons, 167.7ms\n",
      "0: 736x1280 2 persons, 169.6ms\n",
      "0: 736x1280 2 persons, 167.6ms\n",
      "0: 736x1280 2 persons, 160.1ms\n",
      "0: 736x1280 2 persons, 162.8ms\n",
      "0: 736x1280 2 persons, 168.8ms\n",
      "0: 736x1280 2 persons, 166.6ms\n",
      "0: 736x1280 2 persons, 171.0ms\n",
      "0: 736x1280 2 persons, 173.1ms\n",
      "0: 736x1280 2 persons, 165.0ms\n",
      "0: 736x1280 2 persons, 169.0ms\n",
      "0: 736x1280 2 persons, 164.0ms\n",
      "0: 736x1280 2 persons, 170.7ms\n",
      "0: 736x1280 2 persons, 167.9ms\n",
      "0: 736x1280 2 persons, 171.0ms\n",
      "0: 736x1280 2 persons, 151.4ms\n",
      "0: 736x1280 2 persons, 187.8ms\n",
      "0: 736x1280 2 persons, 189.4ms\n",
      "0: 736x1280 1 person, 179.1ms\n",
      "0: 736x1280 1 person, 172.4ms\n",
      "0: 736x1280 1 person, 169.0ms\n",
      "0: 736x1280 2 persons, 172.3ms\n",
      "0: 736x1280 2 persons, 171.3ms\n",
      "0: 736x1280 2 persons, 165.3ms\n",
      "0: 736x1280 2 persons, 166.6ms\n",
      "0: 736x1280 1 person, 160.8ms\n",
      "0: 736x1280 2 persons, 164.0ms\n",
      "0: 736x1280 2 persons, 172.8ms\n",
      "0: 736x1280 1 person, 168.2ms\n",
      "0: 736x1280 1 person, 165.6ms\n",
      "0: 736x1280 2 persons, 154.9ms\n",
      "0: 736x1280 2 persons, 170.5ms\n",
      "0: 736x1280 2 persons, 160.9ms\n",
      "0: 736x1280 1 person, 164.2ms\n",
      "0: 736x1280 1 person, 173.3ms\n",
      "0: 736x1280 1 person, 164.9ms\n",
      "0: 736x1280 1 person, 158.5ms\n",
      "0: 736x1280 1 person, 153.1ms\n",
      "0: 736x1280 2 persons, 173.0ms\n",
      "0: 736x1280 2 persons, 174.9ms\n",
      "0: 736x1280 2 persons, 164.0ms\n",
      "0: 736x1280 1 person, 165.9ms\n",
      "0: 736x1280 2 persons, 164.8ms\n",
      "0: 736x1280 2 persons, 164.6ms\n",
      "0: 736x1280 2 persons, 163.5ms\n",
      "0: 736x1280 1 person, 166.6ms\n",
      "0: 736x1280 1 person, 165.8ms\n",
      "0: 736x1280 1 person, 169.7ms\n",
      "0: 736x1280 1 person, 143.0ms\n",
      "0: 736x1280 1 person, 176.7ms\n",
      "0: 736x1280 1 person, 164.4ms\n",
      "0: 736x1280 1 person, 167.1ms\n",
      "0: 736x1280 1 person, 166.1ms\n",
      "0: 736x1280 1 person, 166.1ms\n",
      "0: 736x1280 1 person, 169.5ms\n",
      "0: 736x1280 1 person, 166.1ms\n",
      "0: 736x1280 1 person, 158.8ms\n",
      "0: 736x1280 1 person, 166.5ms\n",
      "0: 736x1280 1 person, 160.5ms\n",
      "0: 736x1280 1 person, 146.1ms\n",
      "0: 736x1280 2 persons, 173.1ms\n",
      "0: 736x1280 3 persons, 201.2ms\n",
      "0: 736x1280 1 person, 167.1ms\n",
      "0: 736x1280 1 person, 167.4ms\n",
      "0: 736x1280 1 person, 169.9ms\n",
      "0: 736x1280 1 person, 171.2ms\n",
      "0: 736x1280 1 person, 166.8ms\n",
      "0: 736x1280 1 person, 172.3ms\n",
      "0: 736x1280 1 person, 157.2ms\n",
      "0: 736x1280 1 person, 167.8ms\n",
      "0: 736x1280 (no detections), 174.5ms\n",
      "0: 736x1280 (no detections), 172.2ms\n",
      "0: 736x1280 (no detections), 161.1ms\n",
      "0: 736x1280 (no detections), 158.6ms\n",
      "0: 736x1280 (no detections), 169.5ms\n",
      "0: 736x1280 (no detections), 167.5ms\n",
      "0: 736x1280 (no detections), 175.9ms\n",
      "0: 736x1280 (no detections), 163.1ms\n",
      "0: 736x1280 (no detections), 163.1ms\n",
      "0: 736x1280 (no detections), 160.0ms\n",
      "0: 736x1280 (no detections), 164.3ms\n",
      "0: 736x1280 (no detections), 148.1ms\n",
      "0: 736x1280 (no detections), 150.8ms\n",
      "0: 736x1280 (no detections), 166.8ms\n",
      "0: 736x1280 (no detections), 167.6ms\n",
      "0: 736x1280 (no detections), 163.9ms\n",
      "0: 736x1280 (no detections), 167.0ms\n",
      "0: 736x1280 (no detections), 157.1ms\n",
      "0: 736x1280 (no detections), 169.0ms\n",
      "0: 736x1280 (no detections), 159.5ms\n",
      "0: 736x1280 (no detections), 176.0ms\n",
      "0: 736x1280 (no detections), 161.4ms\n",
      "0: 736x1280 (no detections), 159.3ms\n",
      "0: 736x1280 (no detections), 166.7ms\n",
      "0: 736x1280 (no detections), 165.0ms\n",
      "0: 736x1280 (no detections), 157.3ms\n",
      "0: 736x1280 (no detections), 153.8ms\n",
      "0: 736x1280 (no detections), 145.3ms\n",
      "0: 736x1280 (no detections), 166.5ms\n",
      "0: 736x1280 (no detections), 165.3ms\n",
      "0: 736x1280 (no detections), 167.1ms\n",
      "0: 736x1280 (no detections), 161.2ms\n",
      "0: 736x1280 (no detections), 167.7ms\n",
      "0: 736x1280 (no detections), 167.3ms\n",
      "0: 736x1280 (no detections), 172.1ms\n",
      "0: 736x1280 (no detections), 152.4ms\n",
      "0: 736x1280 (no detections), 151.2ms\n",
      "0: 736x1280 (no detections), 164.6ms\n",
      "0: 736x1280 (no detections), 164.9ms\n",
      "0: 736x1280 (no detections), 160.3ms\n",
      "0: 736x1280 (no detections), 154.6ms\n",
      "0: 736x1280 (no detections), 159.4ms\n",
      "0: 736x1280 (no detections), 161.0ms\n",
      "0: 736x1280 (no detections), 164.9ms\n",
      "0: 736x1280 (no detections), 153.4ms\n",
      "0: 736x1280 (no detections), 146.9ms\n",
      "0: 736x1280 (no detections), 166.7ms\n",
      "0: 736x1280 (no detections), 167.9ms\n",
      "0: 736x1280 (no detections), 170.1ms\n",
      "0: 736x1280 (no detections), 158.7ms\n",
      "0: 736x1280 (no detections), 165.6ms\n",
      "0: 736x1280 (no detections), 160.0ms\n",
      "0: 736x1280 (no detections), 158.3ms\n",
      "0: 736x1280 (no detections), 157.5ms\n",
      "0: 736x1280 (no detections), 166.9ms\n",
      "0: 736x1280 (no detections), 161.4ms\n",
      "0: 736x1280 (no detections), 160.3ms\n",
      "0: 736x1280 (no detections), 156.7ms\n",
      "0: 736x1280 (no detections), 161.8ms\n",
      "0: 736x1280 (no detections), 160.3ms\n",
      "0: 736x1280 (no detections), 161.8ms\n",
      "0: 736x1280 1 person, 163.7ms\n",
      "0: 736x1280 (no detections), 162.7ms\n",
      "0: 736x1280 (no detections), 145.5ms\n",
      "0: 736x1280 1 person, 150.0ms\n",
      "0: 736x1280 (no detections), 162.0ms\n",
      "0: 736x1280 (no detections), 160.1ms\n",
      "0: 736x1280 (no detections), 159.8ms\n",
      "0: 736x1280 1 person, 156.6ms\n",
      "0: 736x1280 (no detections), 163.6ms\n",
      "0: 736x1280 1 person, 156.8ms\n",
      "0: 736x1280 (no detections), 159.0ms\n",
      "0: 736x1280 (no detections), 151.9ms\n",
      "0: 736x1280 1 person, 164.0ms\n",
      "0: 736x1280 (no detections), 158.0ms\n",
      "0: 736x1280 (no detections), 158.2ms\n",
      "0: 736x1280 (no detections), 161.7ms\n",
      "0: 736x1280 (no detections), 156.6ms\n",
      "0: 736x1280 (no detections), 156.3ms\n",
      "0: 736x1280 (no detections), 172.6ms\n",
      "0: 736x1280 (no detections), 160.9ms\n",
      "0: 736x1280 (no detections), 147.4ms\n",
      "WARNING  Waiting for stream 0\n",
      "WARNING  Waiting for stream 0\n",
      "WARNING  Waiting for stream 0\n",
      "0: 736x1280 (no detections), 193.2ms\n",
      "0: 736x1280 (no detections), 158.3ms\n",
      "0: 736x1280 (no detections), 158.1ms\n",
      "0: 736x1280 (no detections), 162.0ms\n",
      "0: 736x1280 (no detections), 158.0ms\n",
      "0: 736x1280 (no detections), 155.8ms\n",
      "0: 736x1280 (no detections), 204.0ms\n",
      "0: 736x1280 1 person, 200.7ms\n",
      "0: 736x1280 1 person, 192.7ms\n",
      "0: 736x1280 2 persons, 154.8ms\n",
      "0: 736x1280 (no detections), 156.4ms\n",
      "0: 736x1280 (no detections), 163.6ms\n",
      "0: 736x1280 (no detections), 167.1ms\n",
      "0: 736x1280 (no detections), 166.2ms\n",
      "0: 736x1280 (no detections), 180.7ms\n",
      "0: 736x1280 (no detections), 175.8ms\n",
      "0: 736x1280 (no detections), 180.1ms\n",
      "0: 736x1280 (no detections), 184.6ms\n",
      "0: 736x1280 (no detections), 163.0ms\n",
      "0: 736x1280 (no detections), 170.7ms\n",
      "0: 736x1280 (no detections), 162.0ms\n",
      "0: 736x1280 (no detections), 186.8ms\n",
      "0: 736x1280 (no detections), 147.4ms\n",
      "0: 736x1280 (no detections), 152.9ms\n",
      "0: 736x1280 (no detections), 164.5ms\n",
      "0: 736x1280 (no detections), 165.5ms\n",
      "0: 736x1280 (no detections), 190.8ms\n",
      "0: 736x1280 (no detections), 188.7ms\n",
      "0: 736x1280 (no detections), 156.7ms\n",
      "0: 736x1280 (no detections), 163.5ms\n",
      "0: 736x1280 (no detections), 155.2ms\n",
      "0: 736x1280 (no detections), 152.9ms\n",
      "0: 736x1280 (no detections), 157.2ms\n",
      "0: 736x1280 1 person, 161.4ms\n",
      "0: 736x1280 (no detections), 164.8ms\n",
      "0: 736x1280 1 person, 159.4ms\n",
      "0: 736x1280 1 person, 164.8ms\n",
      "0: 736x1280 1 person, 162.3ms\n",
      "0: 736x1280 1 person, 173.1ms\n",
      "0: 736x1280 1 person, 169.1ms\n",
      "0: 736x1280 1 person, 176.7ms\n",
      "0: 736x1280 1 person, 152.0ms\n",
      "0: 736x1280 1 person, 157.9ms\n",
      "0: 736x1280 (no detections), 159.2ms\n",
      "0: 736x1280 1 person, 155.8ms\n",
      "0: 736x1280 1 person, 154.4ms\n",
      "0: 736x1280 1 person, 153.9ms\n",
      "0: 736x1280 1 person, 151.6ms\n",
      "0: 736x1280 (no detections), 177.5ms\n",
      "0: 736x1280 (no detections), 165.3ms\n",
      "0: 736x1280 (no detections), 210.4ms\n",
      "0: 736x1280 (no detections), 147.0ms\n",
      "0: 736x1280 (no detections), 153.0ms\n",
      "0: 736x1280 (no detections), 161.1ms\n",
      "0: 736x1280 (no detections), 167.8ms\n",
      "0: 736x1280 (no detections), 160.0ms\n",
      "0: 736x1280 (no detections), 158.7ms\n",
      "0: 736x1280 (no detections), 150.5ms\n",
      "0: 736x1280 (no detections), 147.6ms\n",
      "0: 736x1280 (no detections), 160.0ms\n",
      "0: 736x1280 (no detections), 149.6ms\n",
      "0: 736x1280 (no detections), 156.8ms\n",
      "0: 736x1280 (no detections), 154.7ms\n",
      "0: 736x1280 (no detections), 152.6ms\n",
      "0: 736x1280 (no detections), 157.9ms\n",
      "0: 736x1280 (no detections), 160.8ms\n",
      "0: 736x1280 (no detections), 165.3ms\n",
      "0: 736x1280 (no detections), 175.1ms\n",
      "0: 736x1280 (no detections), 159.3ms\n",
      "0: 736x1280 (no detections), 174.9ms\n",
      "0: 736x1280 (no detections), 175.2ms\n",
      "0: 736x1280 (no detections), 151.2ms\n",
      "0: 736x1280 (no detections), 152.4ms\n",
      "0: 736x1280 (no detections), 156.7ms\n",
      "0: 736x1280 (no detections), 156.2ms\n",
      "0: 736x1280 (no detections), 169.3ms\n",
      "0: 736x1280 (no detections), 158.9ms\n",
      "0: 736x1280 (no detections), 190.2ms\n",
      "0: 736x1280 (no detections), 177.9ms\n",
      "0: 736x1280 (no detections), 152.6ms\n",
      "0: 736x1280 (no detections), 158.1ms\n",
      "0: 736x1280 (no detections), 229.5ms\n",
      "0: 736x1280 (no detections), 234.6ms\n",
      "0: 736x1280 (no detections), 229.4ms\n",
      "0: 736x1280 (no detections), 222.9ms\n",
      "0: 736x1280 (no detections), 231.2ms\n",
      "0: 736x1280 (no detections), 211.9ms\n",
      "0: 736x1280 (no detections), 143.4ms\n",
      "Speed: 7.0ms preprocess, 168.3ms inference, 1.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Results saved to \u001b[1mresult\\track\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# from ultralytics import YOLO\n",
    "# # Load the YOLO11 model\n",
    "# model = YOLO(\"yolo11n-pose.pt\")\n",
    "\n",
    "# ## From youtube video\n",
    "# video_link = \"https://youtu.be/OO7XT24AmTY?si=w-_UTTuJr5Fj1Syl\"\n",
    "# results = model.track(video_link, imgsz=32*40, conf=0.55,\n",
    "#                       save=True, show=True, project='./result')  # Tracking with default tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
