{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "######################################################\n",
    "\n",
    "## YOLOv11 options\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "## Mediapipe Pose options\n",
    "model_path = './pose_landmarker_heavy.task'\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "Connections = mp.solutions.pose.POSE_CONNECTIONS\n",
    "LandmarkLabels = mp.solutions.pose.PoseLandmark\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "def calculate_vectors_angle(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    cos_theta = dot_product / (norm_v1 * norm_v2)\n",
    "    angle = np.arccos(cos_theta)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def draw_landmarks(image, landmarks, show_names=False):\n",
    "    for label in LandmarkLabels:\n",
    "        idx = label.value\n",
    "        name = label.name\n",
    "        x = int(landmarks[idx].x * image.shape[1])\n",
    "        y = int(landmarks[idx].y * image.shape[0])\n",
    "        cv2.circle(image, (x, y), 25, (0, 255, 0), 10)  # Hollow circle with thin edge\n",
    "        if show_names:\n",
    "            cv2.putText(image, name, (x-5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    for connection in Connections:\n",
    "        start_idx, end_idx = connection\n",
    "        start_point = (int(landmarks[start_idx].x * image.shape[1]), int(landmarks[start_idx].y * image.shape[0]))\n",
    "        end_point = (int(landmarks[end_idx].x * image.shape[1]), int(landmarks[end_idx].y * image.shape[0]))\n",
    "        cv2.line(image, start_point, end_point, (0, 255, 0), 10)\n",
    "\n",
    "def calculate_angles(landmarks):\n",
    "        angles_labels = []\n",
    "        # Relevant vectors\n",
    "        right_shoulder = landmarks[LandmarkLabels.RIGHT_SHOULDER.value]\n",
    "        right_elbow = landmarks[LandmarkLabels.RIGHT_ELBOW.value]\n",
    "        right_hip = landmarks[LandmarkLabels.RIGHT_HIP.value]\n",
    "\n",
    "        # Vector from shoulder to elbow\n",
    "        v1 = np.array([right_elbow.x - right_shoulder.x, right_elbow.y - right_shoulder.y, right_elbow.z - right_shoulder.z])\n",
    "        # Vector from shoulder to hip\n",
    "        v2 = np.array([right_hip.x - right_shoulder.x, right_hip.y - right_shoulder.y, right_hip.z - right_shoulder.z])\n",
    "        angle = calculate_vectors_angle(v1, v2)\n",
    "\n",
    "        angles_labels.append((\"Right Arm Angle\", angle))\n",
    "\n",
    "        return angles_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def process_image(cv2_image, options):\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        # Load the input image from an image file.\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2_image.astype(np.uint8))\n",
    "        pose_landmarker_result = landmarker.detect(mp_image)\n",
    "        draw_landmarks(cv2_image, pose_landmarker_result.pose_landmarks[0])\n",
    "        angles_labels = calculate_angles(pose_landmarker_result.pose_landmarks[0])\n",
    "        for label, angle in angles_labels:\n",
    "            cv2.putText(cv2_image, f\"{label}: {angle:.2f}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 10, cv2.LINE_AA)\n",
    "        #plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "\n",
    "def process_video(video_path, output_path, options, orientation=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    output_path = os.path.join(output_path, video_path.split(\"/\")[-1].split(\".\")[0]+\".avi\")\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            if orientation == 1:\n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "            elif orientation == 2:\n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "            elif orientation == 3:\n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "            results = model.track(frame, persist=True, conf=0.55)\n",
    "            \n",
    "            # Filter the results to only include boxes with class 0 (person)\n",
    "            filtered_indexes = torch.where(results[0].boxes.cls == 0)\n",
    "            filtered_boxes = None\n",
    "            if filtered_indexes[0].shape[0] > 0:\n",
    "                filtered_boxes = results[0].boxes[filtered_indexes]\n",
    "\n",
    "\n",
    "            # Visualize the results on the frame\n",
    "            filtered_results = results[0]\n",
    "            if filtered_boxes is not None:\n",
    "                filtered_results.boxes = filtered_boxes\n",
    "                annotated_frame = filtered_results.plot()\n",
    "                \n",
    "                # Save the bounding box region of the image in another variable\n",
    "                x1, y1, x2, y2 = map(int, filtered_boxes.xyxy[0])\n",
    "                bounding_box_region = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Display the annotated frame\n",
    "            process_image(bounding_box_region, options)\n",
    "\n",
    "            # Initialize video writer if not already initialized\n",
    "            if 'out' not in locals():\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            # Write the frame to the video\n",
    "            out.write(frame)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 106.2ms\n",
      "Speed: 4.2ms preprocess, 106.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 62.0ms\n",
      "Speed: 1.8ms preprocess, 62.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 92.7ms\n",
      "Speed: 5.5ms preprocess, 92.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 52.7ms\n",
      "Speed: 2.1ms preprocess, 52.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 63.2ms\n",
      "Speed: 3.7ms preprocess, 63.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 81.3ms\n",
      "Speed: 2.6ms preprocess, 81.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 89.3ms\n",
      "Speed: 4.6ms preprocess, 89.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 70.1ms\n",
      "Speed: 1.8ms preprocess, 70.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 59.8ms\n",
      "Speed: 2.1ms preprocess, 59.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 62.1ms\n",
      "Speed: 2.0ms preprocess, 62.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 63.5ms\n",
      "Speed: 2.0ms preprocess, 63.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 53.9ms\n",
      "Speed: 1.7ms preprocess, 53.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 56.6ms\n",
      "Speed: 2.0ms preprocess, 56.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 69.4ms\n",
      "Speed: 1.7ms preprocess, 69.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 62.3ms\n",
      "Speed: 13.9ms preprocess, 62.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 63.8ms\n",
      "Speed: 2.1ms preprocess, 63.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 66.4ms\n",
      "Speed: 2.2ms preprocess, 66.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 68.3ms\n",
      "Speed: 2.1ms preprocess, 68.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 68.7ms\n",
      "Speed: 2.4ms preprocess, 68.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 70.6ms\n",
      "Speed: 2.2ms preprocess, 70.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 77.3ms\n",
      "Speed: 2.2ms preprocess, 77.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 62.3ms\n",
      "Speed: 2.1ms preprocess, 62.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 75.2ms\n",
      "Speed: 2.3ms preprocess, 75.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 73.2ms\n",
      "Speed: 4.2ms preprocess, 73.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 65.7ms\n",
      "Speed: 3.8ms preprocess, 65.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 67.6ms\n",
      "Speed: 2.1ms preprocess, 67.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 72.9ms\n",
      "Speed: 2.6ms preprocess, 72.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 68.8ms\n",
      "Speed: 2.0ms preprocess, 68.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 76.1ms\n",
      "Speed: 2.6ms preprocess, 76.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 50.8ms\n",
      "Speed: 1.7ms preprocess, 50.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 95.8ms\n",
      "Speed: 7.4ms preprocess, 95.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 70.5ms\n",
      "Speed: 2.7ms preprocess, 70.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 89.3ms\n",
      "Speed: 2.7ms preprocess, 89.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 1 cup, 83.2ms\n",
      "Speed: 3.6ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43metc/vidroom1.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_path, options, orientation)\u001b[0m\n\u001b[0;32m     55\u001b[0m     bounding_box_region \u001b[38;5;241m=\u001b[39m frame[y1:y2, x1:x2]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Display the annotated frame\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounding_box_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Initialize video writer if not already initialized\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(cv2_image, options)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_image\u001b[39m(cv2_image, options):\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPoseLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlandmarker\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Load the input image from an image file.\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSRGB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_landmarker_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:226\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__exit__\u001b[1;34m(self, unused_exc_type, unused_exc_value, unused_traceback)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unused_exc_type, unused_exc_value, unused_traceback):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Shuts down the mediapipe vision task instance on exit of the context manager.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m  Raises:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    RuntimeError: If the mediapipe vision task failed to close.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:209\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Shuts down the mediapipe vision task instance.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m  Raises:\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    RuntimeError: If the mediapipe vision task failed to close.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_video('etc/vidroom1.mp4', \"result/\", options, orientation=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
